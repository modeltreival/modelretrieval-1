{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NTCIR-19 ModelRetrieval","text":"<p>Pre-trained Model Retrieval Task</p> <p>Advances in AI have led to a proliferation of pre-trained machine learning models across many domains. However, choosing the most suitable model for a new task remains time-consuming and costly.</p> <p>This task defines a benchmark for efficiently retrieving the pre-trained model that best matches a user's problem, reducing selection overhead and accelerating downstream deployment.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Practitioners currently navigate a vast space of models using manual trial-and-error, which incurs high computational and labor costs. This task aims to:</p> <ul> <li>Encourage development of retrieval algorithms that predict model performance without exhaustive experimentation.</li> <li>Provide a standardized benchmark for fair comparison.</li> <li>Lower the barrier to entry for researchers in this field.</li> </ul> <p>Relation to TREC</p> <p>While TREC 2025's \"Million LLMs Track\" focuses on retrieving LLMs via text queries, our task differs by tackling different model types (BERT, Style Transfer) and scenarios.</p>"},{"location":"data/","title":"Dataset and Resources","text":"<p>We plan to release data under the GPL license. The data will be publicly available online (e.g., GitHub).</p>"},{"location":"data/#subtask-a-language-models","title":"Subtask A: Language Models","text":""},{"location":"data/#queries-tasks","title":"Queries (Tasks)","text":"<ul> <li>Source: Public document classification datasets available on Hugging Face.</li> <li>Details: We will ensure diversity in data size, number of classes, etc. Queries are separated into development and test sets.</li> </ul>"},{"location":"data/#candidate-models","title":"Candidate Models","text":"<ul> <li>Source: Pre-trained BERT models publicly available on Hugging Face.</li> <li>Diversity: Models will differ by structure, parameter size, pre-trained data, etc.</li> </ul>"},{"location":"data/#ground-truth","title":"Ground Truth","text":"<ul> <li>The performance score of each candidate model is determined by organizers by fine-tuning it on the training/validation splits and evaluating on the test split.</li> </ul> <p>Constraint</p> <p>For Subtask A, participants are not allowed to fine-tune the candidate models themselves.</p>"},{"location":"data/#subtask-b-style-transfer-models","title":"Subtask B: Style Transfer Models","text":""},{"location":"data/#query-images","title":"Query Images","text":"<ul> <li>Details: Desired style images will be used as queries. These are generated using an original content image and a style-transfer model.</li> <li>Sets: Queries are separated into development and test sets.</li> </ul>"},{"location":"data/#candidate-models_1","title":"Candidate Models","text":"<ul> <li>Source: Style-transfer models publicly available (e.g., Civitai).</li> <li>Details: Models will vary in the styles they can generate.</li> </ul>"},{"location":"data/#ground-truth_1","title":"Ground Truth","text":"<ul> <li>For every query, we will provide a ground truth model ranked list.</li> </ul>"},{"location":"organizers/","title":"Organizers","text":"<p>If you have any questions, please contact the organizers.</p> <ul> <li>Huu-Long Pham (University of Hyogo) - <code>huulongpham28@gmail.com</code></li> <li>Ryota Mibayashi (Kobe University)</li> <li>Yoshiyuki Shoji (Shizuoka University)</li> <li>Makoto P. Kato (Tsukuba University)</li> <li>Takehiro Yamamoto (University of Hyogo)</li> <li>Yusuke Yamamoto (Nagoya City University)</li> <li>Hiroaki Ohshima (University of Hyogo)</li> </ul>"},{"location":"organizers/#acknowledgements","title":"Acknowledgements","text":"<p>The organizers are supported by two research grants:</p> <ul> <li>JSPS KAKENHI project on machine learning model retrieval.</li> <li>Japan National Institute of Informatics (NII) Open Collaborative Research grant.</li> </ul>"},{"location":"schedule/","title":"Schedule","text":"<p>The tentative schedule for NTCIR-19 ModelRetrieval is as follows:</p> Date Event January 2026 Data release for both subtasks June 2026 Dry-run submissions due July 2026 Formal-run submissions due August 2026 Evaluation results returned to participants August 2026 Task overview release (draft) December 2026 NTCIR-19 Conference and final overview presentation"},{"location":"tasks/","title":"Task Definition","text":"<p>Our benchmark comprises two subtasks: Language Model Retrieval and Image Style Transfer Model Retrieval.</p> Subtask A: Language ModelSubtask B: Style Transfer <p></p> <p></p>"},{"location":"tasks/#subtask-a-language-model-retrieval","title":"Subtask A: Language Model Retrieval","text":"<p>Participants are challenged to predict each model's fine-tuned accuracy for document classification without performing any fine-tuning.</p>"},{"location":"tasks/#background","title":"Background","text":"<p>Selecting the most appropriate pre-trained model for a downstream task is challenging. Traditionally, this requires fine-tuning multiple candidates, which is computationally expensive. We aim to predict this ranking efficiently.</p>"},{"location":"tasks/#problem-definition","title":"Problem Definition","text":"<p>Let \\(D^{train}=\\{(x_{i}^{train},y_{i}^{train})\\}_{i=1}^{M}\\), \\(D^{val}=\\{(x_{i}^{val},y_{i}^{val})\\}_{i=1}^{N}\\), and \\(D^{test}=\\{(x_{i}^{test},y_{i}^{test})\\}_{i=1}^{O}\\) be the training, validation, and test splits of a document classification task.</p> <p>Given a set of \\(k\\) pre-trained models \\(\\{m_{1},...,m_{k}\\}\\), the goal is to rank the models in descending order of their performance on \\(D^{test}\\) after fine-tuning.</p>"},{"location":"tasks/#participant-requirements","title":"Participant Requirements","text":"<ul> <li>Participants must predict model ranking using only \\(D^{train}\\), \\(D^{val}\\), and unlabeled \\(x^{test}\\).</li> <li>Usage of test labels is strictly prohibited.</li> <li>No fine-tuning on candidate models is permitted.</li> </ul>"},{"location":"tasks/#evaluation","title":"Evaluation","text":"<p>Each submitted ranking is compared against the ideal ranking using nDCG@k.</p>"},{"location":"tasks/#subtask-b-image-style-transfer-model-retrieval","title":"Subtask B: Image Style Transfer Model Retrieval","text":"<p>In this subtask, we tackle the problem of retrieving image style transfer models that produce a desired style.</p>"},{"location":"tasks/#background_1","title":"Background","text":"<p>As the number of style-transfer models grows, users face the challenge of selecting a model that best reproduces a desired style. Testing every model is time-consuming.</p>"},{"location":"tasks/#problem-definition_1","title":"Problem Definition","text":"<p>Given a single query image that exemplifies the target style, rank pre-trained style-transfer models by their predicted ability to reproduce that style.</p> <p>Let \\(\\mathcal{M}=\\{m_{1},m_{2},...,m_{K}\\}\\) be a set of candidate style-transfer models, and let \\(x \\in \\mathcal{X}\\) be a query image exhibiting the target style. Rank candidate models in descending order of suitability.</p>"},{"location":"tasks/#participant-requirements_1","title":"Participant Requirements","text":"<ul> <li>Develop a retrieval method that retrieves a style-transfer model capable of reproducing the style of a given query image.</li> <li>For each test query, submit a full ranking of all candidate models.</li> </ul>"},{"location":"tasks/#evaluation_1","title":"Evaluation","text":"<p>Retrieval performance is measured by Mean Reciprocal Rank (MRR) and nDCG@k.</p>"}]}